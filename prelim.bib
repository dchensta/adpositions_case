
@article{srikumar_modeling_2013,
	title = {Modeling {Semantic} {Relations} {Expressed} by {Prepositions}},
	volume = {1},
	issn = {2307-387X},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00223},
	doi = {10.1162/tacl_a_00223},
	abstract = {This paper introduces the problem of predicting semantic relations expressed by prepositions and develops statistical learning models for predicting the relations, their arguments and the semantic types of the arguments. We deﬁne an inventory of 32 relations, building on the word sense disambiguation task for prepositions and collapsing related senses across prepositions. Given a preposition in a sentence, our computational task to jointly model the preposition relation and its arguments along with their semantic types, as a way to support the relation prediction. The annotated data, however, only provides labels for the relation label, and not the arguments and types. We address this by presenting two models for preposition relation labeling. Our generalization of latent structure SVM gives close to 90\% accuracy on relation labeling. Further, by jointly predicting the relation, arguments, and their types along with preposition sense, we show that we can not only improve the relation accuracy, but also signiﬁcantly improve sense prediction accuracy.},
	language = {en},
	urldate = {2020-03-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Srikumar, Vivek and Roth, Dan},
	month = dec,
	year = {2013},
	pages = {231--242},
	file = {SrikumarRo13.pdf:/Users/Daniel/Desktop/Prelim/SrikumarRo13.pdf:application/pdf},
}

@article{zhu_adpositional_nodate,
	title = {Adpositional {Supersenses} for {Mandarin} {Chinese}},
	language = {en},
	author = {Zhu, Yilun and Liu, Yang and Peng, Siyao},
	pages = {6},
	file = {Adpositional Supersenses for Mandarin Chinese.pdf:/Users/Daniel/Desktop/Prelim/Adpositional Supersenses for Mandarin Chinese.pdf:application/pdf},
}

@inproceedings{stowe_linguistic_2019,
	address = {Hong Kong, China},
	title = {Linguistic {Analysis} {Improves} {Neural} {Metaphor} {Detection}},
	url = {https://www.aclweb.org/anthology/K19-1034},
	doi = {10.18653/v1/K19-1034},
	abstract = {In the ﬁeld of metaphor detection, deep learning systems are the ubiquitous and achieve strong performance on many tasks. However, due to the complicated procedures for manually identifying metaphors, the datasets available are relatively small and fraught with complications. We show that using syntactic features and lexical resources can automatically provide additional high-quality training data for metaphoric language, and this data can cover gaps and inconsistencies in metaphor annotation, improving state-of-the-art word-level metaphor identiﬁcation. This novel application of automatically improving training data improves classiﬁcation across numerous tasks, and reconﬁrms the necessity of high-quality data for deep learning frameworks.},
	language = {en},
	urldate = {2020-03-01},
	booktitle = {Proceedings of the 23rd {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Stowe, Kevin and Moeller, Sarah and Michaelis, Laura and Palmer, Martha},
	year = {2019},
	pages = {362--371},
	file = {Kevin Stowe .pdf:/Users/Daniel/Desktop/Prelim/Kevin Stowe .pdf:application/pdf},
}

@article{georgakopoulos_semantic_2018,
	title = {The semantic map model: {State} of the art and future avenues for linguistic research},
	volume = {12},
	issn = {1749818X},
	shorttitle = {The semantic map model},
	url = {http://doi.wiley.com/10.1111/lnc3.12270},
	doi = {10.1111/lnc3.12270},
	abstract = {The semantic map model is relatively new in linguistic research, but it has been intensively used during the past three decades for studying both crosslinguistic and language-specific questions. The goal of the present contribution is to give a comprehensive overview of the model. After introducing the different types of semantic maps, we present the steps involved for building the maps and discuss in more detail the different types of maps and their respective advantages and disadvantages, focusing on the kinds of linguistic generalizations captured. Finally, we provide a thorough survey of the literature on the topic and we sketch future avenues for research in the field.},
	language = {en},
	number = {2},
	urldate = {2020-03-01},
	journal = {Language and Linguistics Compass},
	author = {Georgakopoulos, Thanasis and Polis, Stéphane},
	month = feb,
	year = {2018},
	pages = {e12270},
	file = {Georgakopoulos&Polis_2018_SemanticMaps_Compass.pdf:/Users/Daniel/Desktop/Prelim/Georgakopoulos&Polis_2018_SemanticMaps_Compass.pdf:application/pdf},
}

@article{huang_dissertation_nodate,
	title = {A {Dissertation} submitted to the {Faculty} of the {Graduate} {School} of {Arts} and {Sciences} of {Georgetown} {University} in partial fulfillment of the requirements for the degree of {Doctor} of {Philosophy} in {Linguistics}},
	language = {en},
	author = {Huang, Lihong},
	pages = {367},
	file = {Huang_georgetown_0076D_13857.pdf:/Users/Daniel/Desktop/Prelim/Huang_georgetown_0076D_13857.pdf:application/pdf},
}

@article{schneider_comprehensive_2018,
	title = {Comprehensive {Supersense} {Disambiguation} of {English} {Prepositions} and {Possessives}},
	url = {http://arxiv.org/abs/1805.04905},
	abstract = {Semantic relations are often signaled with prepositional or possessive marking—but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than ﬁne-grained dictionary deﬁnitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker’s lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.},
	language = {en},
	urldate = {2020-03-01},
	journal = {arXiv:1805.04905 [cs]},
	author = {Schneider, Nathan and Hwang, Jena D. and Srikumar, Vivek and Prange, Jakob and Blodgett, Austin and Moeller, Sarah R. and Stern, Aviram and Bitan, Adi and Abend, Omri},
	month = may,
	year = {2018},
	note = {arXiv: 1805.04905},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2018},
	file = {polysem_prep.pdf:/Users/Daniel/Desktop/Prelim/polysem_prep.pdf:application/pdf},
}

@article{liu_linguistic_2019,
	title = {Linguistic {Knowledge} and {Transferability} of {Contextual} {Representations}},
	url = {http://arxiv.org/abs/1903.08855},
	abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of seventeen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.},
	language = {en},
	urldate = {2020-03-01},
	journal = {arXiv:1903.08855 [cs]},
	author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
	month = apr,
	year = {2019},
	note = {arXiv: 1903.08855},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 22 pages, 4 figures; to appear at NAACL 2019},
	file = {ctxtReps.pdf:/Users/Daniel/Desktop/Prelim/ctxtReps.pdf:application/pdf},
}

@misc{noauthor_nert-nlpstreusle_2020,
	title = {nert-nlp/streusle},
	copyright = {CC-BY-SA-4.0},
	url = {https://github.com/nert-nlp/streusle},
	abstract = {STREUSLE: a corpus with comprehensive lexical semantic annotation (multiword expressions, supersenses)},
	urldate = {2020-03-01},
	publisher = {NERT @ Georgetown},
	month = feb,
	year = {2020},
	note = {original-date: 2017-12-05T23:40:54Z},
	keywords = {corpus, lexical-semantics, multiword-expressions, nlp, prepositions, semantics, supersenses},
}

@book{pinkster_oxford_2015,
	title = {The {Oxford} {Latin} {Syntax}: {Volume} 1: {The} {Simple} {Clause}},
	isbn = {978-0-19-928361-3},
	shorttitle = {The {Oxford} {Latin} {Syntax}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199283613.001.0001/acprof-9780199283613},
	language = {en},
	urldate = {2020-06-09},
	publisher = {Oxford University Press},
	author = {Pinkster, Harm},
	month = aug,
	year = {2015},
	doi = {10.1093/acprof:oso/9780199283613.001.0001},
	doi = {10.1093/acprof:oso/9780199283613.001.0001},
	file = {Pinkster - 2015 - The Oxford Latin Syntax Volume 1 The Simple Clau.pdf:/Users/Daniel/Zotero/storage/M4SWY68U/Pinkster - 2015 - The Oxford Latin Syntax Volume 1 The Simple Clau.pdf:application/pdf},
}

@misc{noauthor_bert_nodate,
	title = {{BERT}},
	url = {https://github.com/google-research/bert},
}

@misc{noauthor_multilingual_nodate,
	title = {Multilingual {BERT}},
	url = {https://github.com/google-research/bert/blob/master/multilingual.md},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2020-06-23},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/Daniel/Zotero/storage/F9E8FHK3/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@misc{mccormick_bert_2020,
	title = {{BERT} {Word} {Embeddings} {Tutorial}},
	url = {https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#31-running-bert-on-our-text},
	author = {McCormick, Chris and Ryan, Nick},
	month = may,
	year = {2020},
}

@misc{chi_finding_2020,
	title = {Finding {Cross}-{Lingual} {Syntax} in {Multilingual} {BERT}},
	url = {http://ai.stanford.edu/blog/finding-crosslingual-syntax/},
	journal = {The Stanford AI Lab Blog},
	author = {Chi, Ethan A.},
	month = may,
	year = {2020},
}

@article{manning_emergent_2020,
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1907367117},
	doi = {10.1073/pnas.1907367117},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.},
	language = {en},
	urldate = {2020-07-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	month = jun,
	year = {2020},
	pages = {201907367},
	file = {Manning et al. - 2020 - Emergent linguistic structure in artificial neural.pdf:/Users/Daniel/Zotero/storage/AS59IUV7/Manning et al. - 2020 - Emergent linguistic structure in artificial neural.pdf:application/pdf},
}

@article{chi_finding_2020-1,
	title = {Finding {Universal} {Grammatical} {Relations} in {Multilingual} {BERT}},
	url = {http://arxiv.org/abs/2005.04511},
	abstract = {Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on ﬁnding syntactic trees in neural networks’ internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.},
	language = {en},
	urldate = {2020-07-23},
	journal = {arXiv:2005.04511 [cs]},
	author = {Chi, Ethan A. and Hewitt, John and Manning, Christopher D.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.04511},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7},
	annote = {Comment: To appear in ACL 2020; Farsi typo corrected},
	file = {Chi et al. - 2020 - Finding Universal Grammatical Relations in Multili.pdf:/Users/Daniel/Zotero/storage/5QS26W49/Chi et al. - 2020 - Finding Universal Grammatical Relations in Multili.pdf:application/pdf},
}

@article{pires_how_2019,
	title = {How multilingual is {Multilingual} {BERT}?},
	url = {http://arxiv.org/abs/1906.01502},
	abstract = {In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2019) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-speciﬁc annotations in one language are used to ﬁne-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can ﬁnd translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deﬁciencies affecting certain language pairs.},
	language = {en},
	urldate = {2020-08-06},
	journal = {arXiv:1906.01502 [cs]},
	author = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01502},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Pires et al. - 2019 - How multilingual is Multilingual BERT.pdf:/Users/Daniel/Zotero/storage/Y4IYQM3C/Pires et al. - 2019 - How multilingual is Multilingual BERT.pdf:application/pdf},
}

@article{tenney_what_2019,
	title = {{WHAT} {DO} {YOU} {LEARN} {FROM} {CONTEXT}? {PROBING} {FOR} {SENTENCE} {STRUCTURE} {IN} {CONTEXTUALIZED} {WORD} {REPRESENTATIONS}},
	abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We ﬁnd that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
	language = {en},
	author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R and Das, Dipanjan and Pavlick, Ellie},
	year = {2019},
	pages = {17},
	file = {Tenney et al. - 2019 - WHAT DO YOU LEARN FROM CONTEXT PROBING FOR SENTEN.pdf:/Users/Daniel/Zotero/storage/FJFGZ8ZA/Tenney et al. - 2019 - WHAT DO YOU LEARN FROM CONTEXT PROBING FOR SENTEN.pdf:application/pdf},
}

@article{li_structured_2020,
	title = {Structured {Tuning} for {Semantic} {Role} {Labeling}},
	url = {http://arxiv.org/abs/2005.00496},
	abstract = {Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores. These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models. Introducing the beneﬁts of structure to inform neural models presents a methodological challenge. In this paper, we present a structured tuning framework to improve models using softened constraints only at training time. Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components. We start with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios.},
	language = {en},
	urldate = {2020-08-10},
	journal = {arXiv:2005.00496 [cs]},
	author = {Li, Tao and Jawale, Parth Anand and Palmer, Martha and Srikumar, Vivek},
	month = may,
	year = {2020},
	note = {arXiv: 2005.00496},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted at ACL 2020},
	file = {Li et al. - 2020 - Structured Tuning for Semantic Role Labeling.pdf:/Users/Daniel/Zotero/storage/67K9J87B/Li et al. - 2020 - Structured Tuning for Semantic Role Labeling.pdf:application/pdf},
}

@article{libovicky_language_2020,
	title = {On the {Language} {Neutrality} of {Pre}-trained {Multilingual} {Representations}},
	url = {http://arxiv.org/abs/2004.05160},
	abstract = {Multilingual contextual embeddings, such as multilingual BERT (mBERT) and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the language-neutrality of mBERT with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and in general more informative than aligned static word-type embeddings which are explicitly trained for language neutrality. Contextual embeddings are still by default only moderately language neutral, however, we show two simple methods for achieving stronger language neutrality: ﬁrst, by unsupervised centering of the representation for languages, and second by ﬁtting an explicit projection on small parallel data. In addition, we show how to reach state-of-the-art accuracy on language identiﬁcation and match performance of statistical methods for word alignment in parallel sentences.},
	language = {en},
	urldate = {2020-08-11},
	journal = {arXiv:2004.05160 [cs]},
	author = {Libovický, Jindřich and Rosa, Rudolf and Fraser, Alexander},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.05160},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 11 pages, 3 figures. arXiv admin note: text overlap with arXiv:1911.03310},
	file = {Libovický et al. - 2020 - On the Language Neutrality of Pre-trained Multilin.pdf:/Users/Daniel/Zotero/storage/K882D3EH/Libovický et al. - 2020 - On the Language Neutrality of Pre-trained Multilin.pdf:application/pdf},
}

@article{pimentel_information-theoretic_2020,
	title = {Information-{Theoretic} {Probing} for {Linguistic} {Structure}},
	url = {http://arxiv.org/abs/2004.03061},
	abstract = {The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “know” about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research—plus English—totalling eleven languages. Our implementation is available in https://github.com/ rycolab/info-theoretic-probing.},
	language = {en},
	urldate = {2020-08-11},
	journal = {arXiv:2004.03061 [cs]},
	author = {Pimentel, Tiago and Valvoda, Josef and Maudslay, Rowan Hall and Zmigrod, Ran and Williams, Adina and Cotterell, Ryan},
	month = may,
	year = {2020},
	note = {arXiv: 2004.03061},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted for publication at ACL 2020. This is the camera ready version. Code available in https://github.com/rycolab/info-theoretic-probing},
	file = {Pimentel et al. - 2020 - Information-Theoretic Probing for Linguistic Struc.pdf:/Users/Daniel/Zotero/storage/5FAMERFV/Pimentel et al. - 2020 - Information-Theoretic Probing for Linguistic Struc.pdf:application/pdf},
}

@article{schneider_adposition_2020,
	title = {Adposition and {Case} {Supersenses} v2.5: {Guidelines} for {English}},
	shorttitle = {Adposition and {Case} {Supersenses} v2.5},
	url = {http://arxiv.org/abs/1704.02134},
	abstract = {This document offers a detailed linguistic description of SNACS (Semantic Network of Adposition and Case Supersenses; Schneider et al., 2018), an inventory of 50 semantic labels (“supersenses”) that characterize the use of adpositions and case markers at a somewhat coarse level of granularity, as demonstrated in the STREUSLE corpus (https://github.com/ nert-gu/streusle/; version 4.3 tracks guidelines version 2.5). Though the SNACS inventory aspires to be universal, this document is speciﬁc to English; documentation for other languages will be published separately.},
	language = {en},
	urldate = {2020-08-11},
	journal = {arXiv:1704.02134 [cs]},
	author = {Schneider, Nathan and Hwang, Jena D. and Bhatia, Archna and Srikumar, Vivek and Han, Na-Rae and O'Gorman, Tim and Moeller, Sarah R. and Abend, Omri and Shalev, Adi and Blodgett, Austin and Prange, Jakob},
	month = mar,
	year = {2020},
	note = {arXiv: 1704.02134},
	keywords = {Computer Science - Computation and Language},
	file = {Schneider et al. - 2020 - Adposition and Case Supersenses v2.5 Guidelines f.pdf:/Users/Daniel/Zotero/storage/MRNP2DT2/Schneider et al. - 2020 - Adposition and Case Supersenses v2.5 Guidelines f.pdf:application/pdf},
}

@inproceedings{hwang_double_2017,
	address = {Vancouver, Canada},
	title = {Double {Trouble}: {The} {Problem} of {Construal} in {Semantic} {Annotation} of {Adpositions}},
	shorttitle = {Double {Trouble}},
	url = {http://www.aclweb.org/anthology/S17-1022},
	doi = {10.18653/v1/S17-1022},
	abstract = {We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that an adposition’s lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition’s lexical function so they can be annotated at scale—supporting automatic, statistical processing of domaingeneral language—and discuss how this representation would allow for a simpler inventory of labels.},
	language = {en},
	urldate = {2020-10-12},
	booktitle = {Proceedings of the 6th {Joint} {Conference} on {Lexical} and {Computational}           {Semantics} (*{SEM} 2017)},
	publisher = {Association for Computational Linguistics},
	author = {Hwang, Jena D. and Bhatia, Archna and Han, Na-Rae and O'Gorman, Tim and Srikumar, Vivek and Schneider, Nathan},
	year = {2017},
	pages = {178--188},
	file = {Hwang et al. - 2017 - Double Trouble The Problem of Construal in Semant.pdf:/Users/Daniel/Zotero/storage/P8U4QEJ2/Hwang et al. - 2017 - Double Trouble The Problem of Construal in Semant.pdf:application/pdf},
}

@incollection{kracht_chapter_nodate,
	series = {Text, {Speech} and {Language} {Technology}},
	title = {Chapter 7: {Directionality} {Selection}},
	volume = {29},
	urldate = {2020-08-03},
	booktitle = {Syntax and {Semantics} of {Prepositions}},
	publisher = {Springer International Publishing},
	author = {Kracht, Marcus},
	pages = {101--113},
}

@article{virtanen_multilingual_2019,
	title = {Multilingual is not enough: {BERT} for {Finnish}},
	shorttitle = {Multilingual is not enough},
	url = {http://arxiv.org/abs/1912.07076},
	abstract = {Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at https://turkunlp.org/finbert .},
	language = {en},
	urldate = {2020-10-15},
	journal = {arXiv:1912.07076 [cs]},
	author = {Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.07076},
	keywords = {Computer Science - Computation and Language},
	file = {Virtanen et al. - 2019 - Multilingual is not enough BERT for Finnish.pdf:/Users/Daniel/Zotero/storage/FZEPNQPN/Virtanen et al. - 2019 - Multilingual is not enough BERT for Finnish.pdf:application/pdf},
}

@article{bamman_latin_2020,
	title = {Latin {BERT}: {A} {Contextual} {Language} {Model} for {Classical} {Philology}},
	shorttitle = {Latin {BERT}},
	url = {http://arxiv.org/abs/2009.10053},
	abstract = {We present Latin BERT, a contextual language model for the Latin language, trained on 642.7 million words from a variety of sources spanning the Classical era to the 21st century. In a series of case studies, we illustrate the affordances of this language-speciﬁc model both for work in natural language processing for Latin and in using computational methods for traditional scholarship: we show that Latin BERT achieves a new state of the art for part-of-speech tagging on all three Universal Dependency datasets for Latin and can be used for predicting missing text (including critical emendations); we create a new dataset for assessing word sense disambiguation for Latin and demonstrate that Latin BERT outperforms static word embeddings; and we show that it can be used for semanticallyinformed search by querying contextual nearest neighbors. We publicly release trained models to help drive future work in this space.},
	language = {en},
	urldate = {2020-10-15},
	journal = {arXiv:2009.10053 [cs]},
	author = {Bamman, David and Burns, Patrick J.},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.10053},
	keywords = {Computer Science - Computation and Language},
	file = {Bamman and Burns - 2020 - Latin BERT A Contextual Language Model for Classi.pdf:/Users/Daniel/Zotero/storage/29RJQ776/Bamman and Burns - 2020 - Latin BERT A Contextual Language Model for Classi.pdf:application/pdf},
}

@book{allen_allen_2006,
	title = {Allen and {Greenough}'s {New} {Latin} {Grammar}},
	publisher = {Courier Dover Publications},
	author = {Allen, Joseph Henry and Greenough, James B.},
	year = {2006},
}

@book{de_saint-exupery_little_2000,
	title = {The {Little} {Prince}},
	publisher = {Harcourt, Inc.},
	author = {de Saint-Exupéry, Antoine},
	year = {2000},
}

@book{de_saint-exupery_regulus_2001,
	title = {Regulus},
	publisher = {Mariner Books},
	author = {de Saint-Exupéry, Antoine},
	translator = {Haury, Augusto},
	year = {2001},
}

@book{de_saint-exupery_pikku_1980,
	title = {Pikku {Prinssi}},
	publisher = {WSOY},
	author = {de Saint-Exupéry, Antoine},
	translator = {Packalén, Irma},
	year = {1980},
}

@misc{arvai_k-means_2020,
	title = {K-{Means} {Clustering} in {Python}: {A} {Practical} {Guide}.},
	urldate = {2020-10-01},
	journal = {Real Python},
	author = {Arvai, Kevin},
	month = jul,
	year = {2020},
}

@book{cicero_brutus_1939,
	address = {Cambridge, MA},
	title = {Brutus. {Orator}.},
	volume = {342},
	number = {Loeb Classical Library},
	publisher = {Harvard University Press},
	author = {Cicero, Marcus Tullius},
	translator = {Hendrickson, G.L. and Hubbell, H.M.},
	year = {1939},
}
